\section{Regression}

Regression analysis is a fundamental statistical technique used to model and understand the relationship between a response variable \( Y \) and one or more explanatory variables \( X_1, X_2, \ldots, X_p \). The goal is to estimate a function \( f(X) \) that captures the systematic relationship between \( X \) and \( Y \), while accounting for random variation (error).

\subsection{Additive Error Models}

We often assume an additive model of the form:
\[
Y = f(X) + \varepsilon,
\]
where \( \varepsilon \) is a random error term with
\[
\mathbb{E}[\varepsilon] = 0, \quad \text{and} \quad \text{Var}(\varepsilon) = \sigma^2.
\]
The term \( \varepsilon \) captures the \textbf{irreducible error}—random variation that cannot be explained by the model, even with perfect knowledge of \( f \).

A common goal is to estimate the \textbf{regression function}:
\[
f(X) = \mathbb{E}[Y|X].
\]
This represents the expected value of \( Y \) for a given value of \( X \).

\subsection{Estimating \( f(X) \)}

Given a set of training data \((x_1, y_1), \ldots, (x_n, y_n)\), we want to construct an estimate \(\hat{f}(X)\) that minimizes the expected prediction error:
\[
\mathbb{E}\left[(Y - \hat{f}(X))^2\right].
\]

\subsubsection{K-Nearest Neighbour (KNN) Regression}

A simple non-parametric estimator is the \textbf{K-nearest neighbours (KNN)} regression:
\[
\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in \mathcal{N}_K(x_0)} y_i,
\]
where \(\mathcal{N}_K(x_0)\) is the set of the \(K\) closest training points to \(x_0\).

\paragraph{Example:}
To predict \( f(4) \), we take the average of the \(Y_i\) values for the \(K\) data points whose \(X_i\) values are closest to \(4\):
\[
\hat{f}(4) = \text{avg}\{Y_i : X_i \in \mathcal{N}_K(4)\}.
\]

\paragraph{Advantages and Limitations:}
\begin{itemize}
    \item Performs well when the number of predictors \(p\) is small and sample size \(n\) is large.
    \item Performs poorly when \(p\) is large due to the \textbf{curse of dimensionality}—as \(p\) increases, points become sparse, and distances between them lose meaning.
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=1.0]
\draw[->] (0,0) -- (5,0) node[right] {$X$};
\draw[->] (0,0) -- (0,3) node[above] {$Y$};
\draw[blue, thick] plot[smooth] coordinates {(0.5,0.7) (1,1.2) (1.5,1.6) (2,2) (2.5,1.8) (3,1.4) (3.5,1.1) (4,1)};
\node at (3.3,2.2) {\small $\hat{f}(x)$};
\draw[red, fill=red] (2,2) circle (0.05);
\node[red] at (2,2.3) {$x_0$};
\draw[dashed] (1.5,0) -- (1.5,2.5);
\draw[dashed] (2.5,0) -- (2.5,2.5);
\node at (2,0.3) {\small $\mathcal{N}_K(x_0)$};
\end{tikzpicture}

\textit{Figure: KNN regression—averaging points within a neighbourhood around \(x_0\).}
\end{center}

\subsection{Parametric Models and Linear Regression}

To overcome the limitations of non-parametric methods like KNN, we often use a \textbf{parametric model} that assumes a specific functional form for \( f(X) \):
\[
f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
\]

Even though this form is rarely exactly true, it provides a simple and interpretable approximation.

\subsubsection{Simple Linear Regression}

For a single predictor variable:
\[
Y = \beta_0 + \beta_1 X + \varepsilon.
\]

The goal is to estimate \( \beta_0 \) and \( \beta_1 \) using the training data, such that the \textbf{residual sum of squares (RSS)} is minimized:
\[
\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2.
\]

Setting the partial derivatives of RSS with respect to \(\beta_0\) and \(\beta_1\) equal to zero yields the least-squares estimates.

\paragraph{Population Regression Line Example:}
\[
Y = 2 + 3X + \varepsilon \quad \Rightarrow \quad f(X) = 2 + 3X.
\]
Here, \(\beta_0 = 2\) and \(\beta_1 = 3\) are unbiased estimates of the true relationship between \(X\) and \(Y\).

\begin{center}
\begin{tikzpicture}[scale=1.0]
\draw[->] (0,0) -- (5,0) node[right] {$X$};
\draw[->] (0,0) -- (0,3) node[above] {$Y$};

% regression line
\draw[blue, thick] (0.5,0.8) -- (4.5,2.5) node[right] {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$};

% data points and residuals
\foreach \x/\y in {0.7/0.9, 1.2/1.4, 2/1.9, 3/2.2, 3.8/2.6} {
  \draw[red, fill=red] (\x,\y) circle (0.05);
  \draw[dashed, gray] (\x,\y) -- (\x,{0.5+0.4*\x});
}

\node at (3.5,0.6) {\small Residuals: $e_i = y_i - \hat{y}_i$};
\end{tikzpicture}

\textit{Figure: Simple linear regression with fitted line and residuals.}
\end{center}

\subsubsection{Standard Errors and Confidence Intervals}

The variance of the error term is denoted \(\sigma^2 = \text{Var}(\varepsilon)\).  
The estimated variance from data is:
\[
\widehat{\sigma}^2 = \frac{\text{RSS}}{n - 2}.
\]

The \textbf{standard error} of \(\hat{\beta}_1\) measures the uncertainty of the slope estimate.

A 95\% confidence interval for \(\beta_1\) is:
\[
[\hat{\beta}_1 - 2 \cdot SE(\hat{\beta}_1), \; \hat{\beta}_1 + 2 \cdot SE(\hat{\beta}_1)].
\]

\subsection{Hypothesis Testing}

We often test whether there is a significant relationship between \(X\) and \(Y\):

\[
\begin{aligned}
H_0 &: \beta_1 = 0 \quad \text{(no relationship)} \\
H_a &: \beta_1 \neq 0 \quad \text{(some relationship)}
\end{aligned}
\]

The test statistic is:
\[
t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)},
\]
which follows a \(t\)-distribution with \(n - 2\) degrees of freedom under \(H_0\).

The \textbf{p-value} is the probability of observing a \( |t| \) as large as the one obtained, assuming \(H_0\) is true.  
A small p-value (typically \(< 0.05\)) leads to rejection of \(H_0\).

\subsection{Assessing Model Accuracy}

\subsubsection{Residual Standard Error (RSE)}

The RSE provides an estimate of the standard deviation of the residuals:
\[
RSE = \sqrt{\frac{RSS}{n - 2}}.
\]

\subsubsection{The \(R^2\) Statistic}

The \(R^2\) statistic measures the proportion of variability in \(Y\) explained by the model:
\[
R^2 = 1 - \frac{RSS}{TSS},
\]
where
\[
TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2.
\]
Here, \(TSS\) measures the total variance in \(Y\), and \(RSS\) measures the remaining variability after regression.

\paragraph{Properties:}
\begin{itemize}
    \item \(R^2\) typically lies between 0 and 1.
    \item It can be negative for models that fit worse than a horizontal line at \(\bar{y}\).
    \item \(R^2\) is independent of the units of \(Y\).
\end{itemize}

\subsection{Bias–Variance Tradeoff}

When fitting regression models, we balance two sources of error:
\[
\text{Expected Test Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}.
\]
Parametric models typically have higher bias but lower variance, while non-parametric models like KNN have lower bias but higher variance.

\begin{center}
\begin{tikzpicture}[scale=1.0]
% Axes
\draw[->] (0,0) -- (5.2,0) node[right] {Model Flexibility};
\draw[->] (0,0) -- (0,3) node[above] {Error};

% Curves
\draw[red, thick, domain=0:4.5, smooth] plot (\x,{0.5+1.8*exp(-0.8*\x)}) node[right] {\textcolor{red}{Bias$^2$}};
\draw[blue, thick, domain=0:4.5, smooth] plot (\x,{0.2+0.1*\x}) node[right] {\textcolor{blue}{Variance}};
\draw[black, thick, domain=0:4.5, smooth] plot (\x,{0.7+0.1*\x+1.8*exp(-0.8*\x)}) node[right] {Test Error};

% Annotations
\node[align=left, text width=3cm] at (1.3,2.5) {\small \textbf{High Bias}\\Low Variance};
\draw[->, thick, gray!70] (1.3,2.3) -- (0.8,1.7);

\node[align=left, text width=3cm] at (4.2,1.9) {\small \textbf{Low Bias}\\High Variance};
\draw[->, thick, gray!70] (4.0,1.8) -- (3.5,1.4);

% Optional labels for minimum point
\node at (2.4,0.7) {\small $\star$};
\node at (2.4,0.5) {\small Minimum Test Error};

\end{tikzpicture}

\textit{Figure: Bias–variance tradeoff showing how total test error depends on model flexibility.}
\end{center}


\subsection{Software Note: R and Quarto}

When working with R in VSCode, you can compile regression analyses and visualizations within \texttt{.qmd} (Quarto) files, which support both code and markdown text for reproducible reports.



% Regression Notes (Version with expanded Model Selection only)



\section{Multivariate Regression}
Multivariate (multiple) linear regression models the relationship between a response variable \(Y\) and multiple predictors \(X_1, X_2, \dots, X_p\) using an additive model:
\begin{equation}
Y = \beta_0 + \sum_{i=1}^p \beta_i X_i + \varepsilon.
\end{equation}
The goal is to estimate coefficients \(\beta_i\) that minimize the Residual Sum of Squares (RSS):
\begin{equation}
RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2.
\end{equation}

\subsection{Design Matrix and Hat Matrix}
In matrix notation, the model can be written:
\begin{equation}
\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\end{equation}
The fitted values are given by:
\begin{equation}
\hat{\mathbf{y}} = H \mathbf{y}, \quad \text{where } H = X(X^TX)^{-1}X^T.
\end{equation}
The matrix \(H\) is the \textbf{hat matrix}, projecting the data onto the regression hyperplane.

\subsection{Correlation and Visualization}
To assess relationships between predictors and the response, we often compute the sample correlation:
\begin{equation}
\text{Cor}(X, Y) = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}.
\end{equation}
Heatmaps of correlation matrices are helpful for identifying linear associations.

\subsection{Inference: t-statistics and p-values}
To test whether individual predictors have a relationship with \(Y\), we test:
\begin{equation}
H_0: \beta_j = 0, 
\qquad 
H_a: \beta_j \neq 0.
\end{equation}
We compute the t-statistic:
\begin{equation}
t_j = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}.
\end{equation}
A small p-value indicates evidence against \(H_0\).

\subsection{F-statistic for Overall Significance}
The F-test evaluates whether the model provides a better fit than a constant-only model:
\begin{equation}
H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0.
\end{equation}
Large \(F\) values suggest the model explains significant variation.

\subsection{Variable Selection}
When many predictors are available, we must decide which ones to include.
Adding more variables always reduces RSS and always increases \(R^2\), so we need principled methods.

\subsubsection{Forward Selection}

\begin{enumerate}
\item Start with the intercept-only model.
\item Add the predictor that most improves the model (largest reduction in RSS or improvement in F-statistic).
\item Continue adding predictors until no significant improvement occurs.
\end{enumerate}

\subsubsection{Backward Selection}

\begin{enumerate}
\item Start with the full model.
\item Remove the predictor with the highest p-value (least significant).
\item Continue removing predictors until all remaining variables are significant.
\end{enumerate}

\subsubsection{Stepwise (Mixed) Selection}
A combination of forward and backward selection:
\begin{itemize}
\item Add the best candidate predictor.
\item Then check whether any included predictors should be removed.
\end{itemize}
This continues until no improvement is possible.

\subsubsection{Model Selection Rationale}
Variable selection helps:
\begin{itemize}
\item Improve interpretability.
\item Reduce variance of coefficient estimates.
\item Avoid multicollinearity.
\item Prevent overfitting.
\end{itemize}
P-values alone are not sufficient; the overall model validity should be checked using the F-statistic.

\subsection{Model Fit: RSE and \(R^2\)}
\textbf{Residual Standard Error (RSE)} estimates the standard deviation of the error term:
\begin{equation}
RSE = \sqrt{\frac{RSS}{n - p - 1}}.
\end{equation}
\textbf{\(R^2\)} measures the fraction of variance explained:
\begin{equation}
R^2 = 1 - \frac{RSS}{TSS} = \text{Cor}(Y, \hat{Y})^2.
\end{equation}
Note: \(R^2\) always increases when adding predictors.

\subsection{Prediction and Uncertainty}
Predictions come with two types of uncertainty:
\begin{itemize}
\item \textbf{Confidence intervals}: uncertainty in the mean response.
\item \textbf{Prediction intervals}: includes irreducible error; always wider.
\end{itemize}
Example in R:
\begin{verbatim}
model <- lm(y ~ x1 + x2)
predict(model, newdata, interval = "confidence")
\end{verbatim}

\subsection{Interaction Terms}
Including interactions allows the effect of one predictor to depend on another:
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2 + \varepsilon.
\end{equation}

\subsection{Polynomial Regression}
A polynomial model remains linear in the parameters:
\begin{equation}
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \dots + \beta_d X^d + \varepsilon.
\end{equation}

\subsection{Residuals and Nonlinearity}
Examining residuals helps diagnose model misfit. If residuals show curvature, a polynomial or nonlinear transformation may be appropriate.

\subsection{Correlated Error Terms}
Assuming errors are independent is critical. Correlated errors lead to underestimation of standard errors.

\subsection{Heteroskedasticity}
If error variance is not constant:
\begin{equation}
\operatorname{Var}(\varepsilon_i) = \sigma_i^2 \neq \sigma^2,
\end{equation}
then confidence intervals and standard errors become unreliable.

\subsection{Outliers and Leverage Points}
Outliers have unusual \(Y\) values; leverage points have unusual \(X\) values.
Leverage for observation \(i\):
\begin{equation}
h_{ii} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum (x_i - \bar{x})^2}.
\end{equation}
High-leverage points can strongly affect regression lines.

\subsection{Collinearity}
When predictors are highly correlated, it becomes difficult to distinguish their individual effects.
Consequences include:
\begin{itemize}
\item Inflated standard errors
\item Small t-statistics
\item Unstable coefficient estimates
\end{itemize}
The Variance Inflation Factor (VIF) detects multicollinearity.




